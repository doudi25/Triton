{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMl79MZG3OSyqNc0v2AgtNe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doudi25/Triton/blob/main/DyT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "G3iluBuIgJ2m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define our custom PyTorch layer\n",
        "class DyT(nn.Module):\n",
        "    def __init__(self, scaling_factor: float):\n",
        "        super().__init__()\n",
        "        self.scale = scaling_factor  # Scaling factor for the tanh activation\n",
        "        # Parameters initialized as float64 for precision\n",
        "        self.weight = nn.Parameter(torch.ones(1, dtype=torch.float64), requires_grad=True)\n",
        "        self.bias = nn.Parameter(torch.zeros(1, dtype=torch.float64), requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Simple forward pass: weight * tanh(scale * x) + bias\n",
        "        return self.weight * F.tanh(self.scale * x) + self.bias"
      ],
      "metadata": {
        "id": "-fW6nu1vgQGB"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def tanh(x):\n",
        "    exp_x = tl.exp(x)\n",
        "    exp_neg_x = 1.0 / exp_x\n",
        "    return (exp_x - exp_neg_x) / (exp_x + exp_neg_x)"
      ],
      "metadata": {
        "id": "ETK5-Z2B3Pm4"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _forward_kernel(x_ptr,normalized_ptr,stride_m,stride_n,scale,weight_ptr,\n",
        "                    bias_ptr,m,n,BLOCK_SIZE_M:tl.constexpr,BLOCK_SIZE_N:tl.constexpr,num_warps=64):\n",
        "    # Get program IDs for the block grid\n",
        "    pid_m = tl.program_id(axis=0)\n",
        "    pid_n = tl.program_id(axis=1)\n",
        "\n",
        "    # Calculate offsets for our block\n",
        "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
        "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
        "\n",
        "    # Create a mask to avoid out-of-bounds access\n",
        "    mask = (offs_m[:, None] < m) & (offs_n[None, :] < n)\n",
        "\n",
        "    # Pointer arithmetic to load our chunk of input\n",
        "    x_ptrs = x_ptr + offs_m[:, None] * stride_m + offs_n[None, :] * stride_n\n",
        "    x = tl.load(x_ptrs, mask=mask)\n",
        "\n",
        "    # Load weight and bias (these are scalars)\n",
        "    weight = tl.load(weight_ptr)\n",
        "    bias = tl.load(bias_ptr)\n",
        "\n",
        "    # Compute the output: weight * tanh(scale * x) + bias\n",
        "    out = weight * tanh(scale * x) + bias  # Note: using Triton's tanh\n",
        "\n",
        "    # Store the result in our output tensor\n",
        "    out_ptrs = normalized_ptr + offs_m[:, None] * stride_m + offs_n[None, :] * stride_n\n",
        "    tl.store(out_ptrs, out, mask=mask)"
      ],
      "metadata": {
        "id": "Oe3KsJSp1kIE"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _forward_DyT(x:torch.tensor,scale:float,weight:torch.tensor,bias:torch.tensor):\n",
        "  assert x.is_cuda and weight.is_cuda, \"Inputs must be on CUDA!\"\n",
        "  assert x.is_contiguous(), \"Input tensor must be contiguous for Triton.\"\n",
        "\n",
        "  m, n = x.shape  # Get input dimensions\n",
        "  # Create an empty tensor to hold the output\n",
        "  normalized = torch.empty_like(x, device=x.device, dtype=x.dtype)\n",
        "\n",
        "  # Define block sizes for the grid (tuned for performance)\n",
        "  BLOCK_SIZE_M = 32\n",
        "  BLOCK_SIZE_N = 64\n",
        "  grid = (triton.cdiv(m, BLOCK_SIZE_M), triton.cdiv(n, BLOCK_SIZE_N))\n",
        "\n",
        "  # Launch the kernel with our grid\n",
        "  _forward_kernel[grid](\n",
        "        x, normalized, x.stride(0), x.stride(1), scale, weight, bias,\n",
        "        m, n, BLOCK_SIZE_M, BLOCK_SIZE_N)\n",
        "  return normalized"
      ],
      "metadata": {
        "id": "8-McE3qUh4br"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _backward_kernel(x_ptr,dout_ptr,dweight_ptr,dbias_ptr,dx_ptr,stride_m,stride_n,\n",
        "                     scale,weight_ptr,bias_ptr,m,n,BLOCK_SIZE_M:tl.constexpr,BLOCK_SIZE_N:tl.constexpr,num_warps=64):\n",
        "  # Same grid setup as forward\n",
        "  pid_m = tl.program_id(axis=0)\n",
        "  pid_n = tl.program_id(axis=1)\n",
        "  offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
        "  offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
        "  mask = (offs_m[:, None] < m) & (offs_n[None, :] < n)\n",
        "\n",
        "  # Load input and gradient of output (dout)\n",
        "  x_ptrs = x_ptr + offs_m[:, None] * stride_m + offs_n[None, :] * stride_n\n",
        "  dout_ptrs = dout_ptr + offs_m[:, None] * stride_m + offs_n[None, :] * stride_n\n",
        "  x = tl.load(x_ptrs, mask=mask)\n",
        "  dout = tl.load(dout_ptrs, mask=mask)\n",
        "\n",
        "  # Load weight and bias\n",
        "  weight = tl.load(weight_ptr)\n",
        "  bias = tl.load(bias_ptr)  # Not really used here, but kept for consistency\n",
        "\n",
        "  # Compute intermediate tanh for gradient calculations\n",
        "  tan = tanh(scale * x)\n",
        "  # Gradients: dweight and dbias are sums over the block\n",
        "  dweight = tl.sum(dout * tan)\n",
        "  dbias = tl.sum(dout)\n",
        "\n",
        "  # Gradient w.r.t. input: dout * weight * scale * (1 - tanh^2)\n",
        "  dx = dout * weight * scale * (1 - tan * tan)\n",
        "\n",
        "  # Accumulate gradients atomically since multiple blocks might write to these\n",
        "  tl.atomic_add(dweight_ptr, dweight)\n",
        "  tl.atomic_add(dbias_ptr, dbias)\n",
        "\n",
        "  # Store the input gradient\n",
        "  dx_ptrs = dx_ptr + offs_m[:, None] * stride_m + offs_n[None, :] * stride_n\n",
        "  tl.store(dx_ptrs, dx, mask=mask)"
      ],
      "metadata": {
        "id": "Gb9OZ9619EFW"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _backward_DyT(x:torch.tensor,dout:torch.tensor,scale:float,weight:torch.tensor,bias:torch.tensor):\n",
        "  assert x.is_cuda and x.is_contiguous()\n",
        "  assert dout.is_cuda\n",
        "  dout = dout.is_contiguous() if not dout.is_contiguous() else dout\n",
        "  dweight = torch.empty(1,device='cuda',dtype=x.dtype)\n",
        "  dbias = torch.empty(1,device='cuda',dtype=x.dtype)\n",
        "  dx = torch.empty_like(x,device='cuda',dtype=x.dtype)\n",
        "  m,n = x.shape\n",
        "  BLOCK_SIZE_M = 32\n",
        "  BLOCK_SIZE_N = 64\n",
        "  grid = (triton.cdiv(m,BLOCK_SIZE_M),triton.cdiv(n,BLOCK_SIZE_N))\n",
        "  _backward_kernel[grid](x,dout,dweight,dbias,dx,x.stride(0),\n",
        "                         x.stride(1),scale,weight,bias,m,n,BLOCK_SIZE_M,BLOCK_SIZE_N)\n",
        "  return (dx,dweight,dbias)"
      ],
      "metadata": {
        "id": "Dx73xqgF5EJM"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test it out!\n",
        "layer = DyT(0.5).to('cuda')  # Create our layer and move to GPU\n",
        "input = torch.rand((512, 512), device='cuda', dtype=torch.float64, requires_grad=True)\n",
        "\n",
        "# PyTorch forward and backward\n",
        "out = layer(input)\n",
        "loss = out.sum()\n",
        "loss.backward()\n",
        "\n",
        "# Triton forward and backward\n",
        "out_triton = _forward_DyT(input, layer.scale, layer.weight, layer.bias)\n",
        "grads = _backward_DyT(input, torch.ones_like(input), layer.scale, layer.weight, layer.bias)"
      ],
      "metadata": {
        "id": "Kk-Xs_x53SAU"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Output match between PyTorch and Triton:\", torch.allclose(out, out_triton))\n",
        "\n",
        "print(\"Input gradient match:\", torch.allclose(input.grad, grads[0]))\n",
        "# allowing a relative tolerance of 1e-2 for floating-point differences\n",
        "print(\"Weight gradient match (tol=1e-2):\", torch.allclose(layer.weight.grad, grads[1], rtol=1e-2))\n",
        "# also with a tolerance of 1e-2\n",
        "print(\"Bias gradient match (tol=1e-2):\", torch.allclose(layer.bias.grad, grads[2], rtol=1e-2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv_thaf-FXK9",
        "outputId": "cb820b10-1e5a-4590-e1c9-8af1ae5dc06d"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output match between PyTorch and Triton: True\n",
            "Input gradient match: True\n",
            "Weight gradient match (tol=1e-2): True\n",
            "Bias gradient match (tol=1e-2): True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EYGbJjk4B3qU"
      }
    }
  ]
}