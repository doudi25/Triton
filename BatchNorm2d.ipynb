{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP/z+vQDEITRodbjLxXyt2c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doudi25/Triton/blob/main/BatchNorm2d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "0wITrfvh5uMj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def batchnorm2d_kernel(img_ptr,stride_i0,stride_i1,stride_i2,stride_i3,gemma_ptr,beta_ptr,eps,\n",
        "                       bs,c,h,w,BLOCK_SIZE_ROW:tl.constexpr,BLOCK_SIZE_COL:tl.constexpr,\n",
        "                       num_warps=8):\n",
        "  pid_c = tl.program_id(axis=0)\n",
        "  offs_row = tl.arange(0,BLOCK_SIZE_ROW)\n",
        "  offs_col = tl.arange(0,BLOCK_SIZE_COL)\n",
        "  mask = (offs_row[:,None] < h) & (offs_col[None,:] < w)\n",
        "  mu = 0.0\n",
        "  var = 0.0\n",
        "  gemma = tl.load(gemma_ptr+pid_c)\n",
        "  beta = tl.load(beta_ptr+pid_c)\n",
        "  for step in range(bs):\n",
        "    img_ptrs = img_ptr + step * stride_i0 + pid_c * stride_i1 + offs_row[:,None] * stride_i2 + offs_col[None,:] * stride_i3\n",
        "    img_chunk = tl.load(img_ptrs,mask=mask)\n",
        "    mu += tl.sum(img_chunk)/( h * w )\n",
        "  mu = mu / bs\n",
        "  for step in range(bs):\n",
        "    img_ptrs = img_ptr + step * stride_i0 + pid_c * stride_i1 + offs_row[:,None] * stride_i2 + offs_col[None,:] * stride_i3\n",
        "    img_chunk = tl.load(img_ptrs,mask=mask)\n",
        "    var += tl.sum((img_chunk-mu)*(img_chunk-mu))/( h * w )\n",
        "  var = var / bs\n",
        "  for step in range(bs):\n",
        "    img_ptrs = img_ptr + step * stride_i0 + pid_c * stride_i1 + offs_row[:,None] * stride_i2 + offs_col[None,:] * stride_i3\n",
        "    img_chunk = tl.load(img_ptrs,mask=mask)\n",
        "    img_chunk = ((img_chunk-mu)* tl.rsqrt(var+eps)) * gemma + beta\n",
        "    tl.store(img_ptrs,img_chunk)"
      ],
      "metadata": {
        "id": "PuRCT7jwTWts"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batchnorm2d(image:torch.tensor,gemma,beta,eps):\n",
        "  assert image.is_cuda and image.is_contiguous()\n",
        "  assert image.ndim == 4\n",
        "  bs,c,h,w = image.shape\n",
        "  BLOCK_SIZE_ROW = triton.next_power_of_2(h)\n",
        "  BLOCK_SIZE_COL = triton.next_power_of_2(w)\n",
        "  grid = (c,)\n",
        "  batchnorm2d_kernel[grid](image,image.stride(0),image.stride(1),\n",
        "                           image.stride(2),image.stride(3),gemma,\n",
        "                           beta,eps,bs,c,h,w,BLOCK_SIZE_ROW,BLOCK_SIZE_COL)\n",
        "  return image"
      ],
      "metadata": {
        "id": "8mwnM92UCF0o"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.randn((2,16,32,32),device='cuda')\n",
        "layer = torch.nn.BatchNorm2d(16).to('cuda')\n",
        "out = layer(image)\n",
        "out_triton = batchnorm2d(image,layer.weight,layer.bias,layer.eps)\n",
        "torch.allclose(out,out_triton)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4ciSGxnW77v",
        "outputId": "de7b1fde-b531-4fd7-e5a3-4bedab00c97c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "def benchmark(fn, *args, warmup=8, steps=128):\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "    for _ in range(warmup):\n",
        "        fn(*args)\n",
        "    torch.cuda.synchronize()\n",
        "    start.record()\n",
        "    for _ in range(steps):\n",
        "        fn(*args)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return start.elapsed_time(end) / steps\n",
        "\n",
        "# Define shapes for benchmarking\n",
        "shapes = [\n",
        "    (4, 8, 64, 64),\n",
        "    (4, 16, 32, 32),\n",
        "    (8, 8, 64, 64),\n",
        "    (16, 16, 32, 32),\n",
        "]\n",
        "\n",
        "# Create random inputs\n",
        "images = [torch.randn(shape, device='cuda', dtype=torch.float32) for shape in shapes]\n",
        "gemmas = [torch.ones(shape[1], device='cuda', dtype=torch.float32) for shape in shapes]\n",
        "betas = [torch.zeros(shape[1], device='cuda', dtype=torch.float32) for shape in shapes]\n",
        "eps = 1e-5\n",
        "layers = [torch.nn.BatchNorm2d(shape[1],device='cuda',dtype=torch.float32) for shape in shapes]\n",
        "\n",
        "triton_time = [benchmark(batchnorm2d, image, gemma, beta, eps) for image, gemma, beta in zip(images, gemmas, betas)]\n",
        "\n",
        "layers = [torch.nn.BatchNorm2d(shape[1], device='cuda', dtype=torch.float32) for shape in shapes]\n",
        "\n",
        "for layer in layers:\n",
        "    layer.eval()\n",
        "\n",
        "torch_time = [benchmark(lambda x, g, b, layer=layer: layer(x), image, gemma, beta)\n",
        "              for layer, image, gemma, beta in zip(layers, images, gemmas, betas)]\n",
        "\n",
        "table = PrettyTable()\n",
        "table.field_names = [\"Shape\", \"Triton Time (ms)\", \"PyTorch Time (ms)\"]\n",
        "\n",
        "for shape, triton_t, torch_t in zip(shapes, triton_time, torch_time):\n",
        "    table.add_row([shape, f\"{triton_t:.4f}\", f\"{torch_t:.4f}\"])\n",
        "\n",
        "print(table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POpE_0HcZMg_",
        "outputId": "2953ce80-3091-4b85-8160-8872b6f8d180"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+------------------+-------------------+\n",
            "|      Shape       | Triton Time (ms) | PyTorch Time (ms) |\n",
            "+------------------+------------------+-------------------+\n",
            "|  (4, 8, 64, 64)  |      0.0382      |       0.0446      |\n",
            "| (4, 16, 32, 32)  |      0.0355      |       0.0505      |\n",
            "|  (8, 8, 64, 64)  |      0.0533      |       0.0446      |\n",
            "| (16, 16, 32, 32) |      0.0486      |       0.0445      |\n",
            "+------------------+------------------+-------------------+\n"
          ]
        }
      ]
    }
  ]
}