{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNcHQydl4dgUxhvpXH+we+e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doudi25/Triton/blob/main/GroupNorm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yIsnK8z5pVW5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def groupnorm_kernel(img_ptr,gamma_ptr,bias_ptr,eps, stride_i0, stride_i1, stride_i2,\n",
        "                      stride_i3, stride_i4, bs, n_grs, c, h, w, BLOCK_SIZE_ROW:tl.constexpr,\n",
        "                      BLOCK_SIZE_COL:tl.constexpr,num_warps=4):\n",
        "  pid_b = tl.program_id(axis=0)\n",
        "  pid_gr = tl.program_id(axis=1)\n",
        "  offs_row = tl.arange(0,BLOCK_SIZE_ROW)\n",
        "  offs_col = tl.arange(0,BLOCK_SIZE_COL)\n",
        "  mask = (offs_row[:,None]<h) & (offs_col[None,:] < w)\n",
        "  mu = 0.0\n",
        "  var = 0.0\n",
        "  for step in range(c//n_grs):\n",
        "    img_ptrs = img_ptr + pid_b * stride_i0 + pid_gr * stride_i1 + step * stride_i2 + offs_row[:,None] * stride_i3 + offs_col[None,:] * stride_i4\n",
        "    img_chunk = tl.load(img_ptrs,mask=mask)\n",
        "    mu += tl.sum(img_chunk) / (h * w)\n",
        "  mu = mu / ( c // n_grs)\n",
        "  for step in range(c//n_grs):\n",
        "    img_ptrs = img_ptr + pid_b * stride_i0 + pid_gr * stride_i1 + step * stride_i2 + offs_row[:,None] * stride_i3 + offs_col[None,:] * stride_i4\n",
        "    img_chunk = tl.load(img_ptrs,mask=mask)\n",
        "    var += tl.sum((img_chunk -mu) * (img_chunk -mu)) / (h * w)\n",
        "  var = var / ( c // n_grs)\n",
        "  for step in range(c//n_grs):\n",
        "    img_ptrs = img_ptr + pid_b * stride_i0 + pid_gr * stride_i1 + step * stride_i2 + offs_row[:,None] * stride_i3 + offs_col[None,:] * stride_i4\n",
        "    img_chunk = tl.load(img_ptrs,mask=mask)\n",
        "    gamma = tl.load(gamma_ptr + pid_gr * (c//n_grs) + step)\n",
        "    bias = tl.load(bias_ptr + pid_gr * (c//n_grs) + step)\n",
        "    img_chunk = ( (img_chunk - mu) * tl.rsqrt(var + eps) )* gamma + bias\n",
        "    tl.store(img_ptrs,img_chunk,mask=mask)\n"
      ],
      "metadata": {
        "id": "L_jLZ2Djs01P"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def groupnorm(image:torch.tensor,n_grs:int,gamma:float,bias:float,eps:float):\n",
        "  assert image.is_cuda and image.is_contiguous()\n",
        "  assert image.ndim == 4\n",
        "  bs,c,h,w = image.shape\n",
        "  assert c % n_grs == 0\n",
        "  image = image.view(bs,n_grs,c//n_grs,h,w)\n",
        "  BLOCK_SIZE_ROW = triton.next_power_of_2(h)\n",
        "  BLOCK_SIZE_COL = triton.next_power_of_2(w)\n",
        "  grid = (bs,n_grs)\n",
        "  groupnorm_kernel[grid](image,gamma,bias,eps,image.stride(0),image.stride(1),\n",
        "                         image.stride(2),image.stride(3),image.stride(4),bs,n_grs\n",
        "                         ,c,h,w,BLOCK_SIZE_ROW,BLOCK_SIZE_COL)\n",
        "  return image.view(bs,c,h,w)"
      ],
      "metadata": {
        "id": "b2ltvxwYqJkt"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normlayer = nn.GroupNorm(3,12).to('cuda')\n",
        "image = torch.rand((4,12,16,16),device='cuda',dtype=torch.float32)\n",
        "img_norm = normlayer(image)\n",
        "img_norm_triton = groupnorm(image,normlayer.num_groups,normlayer.weight,normlayer.bias,normlayer.eps)"
      ],
      "metadata": {
        "id": "l1s6TBivwqXN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "def benchmark(fn, *args, warmup=8, steps=128):\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "    for _ in range(warmup):\n",
        "        fn(*args)\n",
        "    torch.cuda.synchronize()\n",
        "    start.record()\n",
        "    for _ in range(steps):\n",
        "        fn(*args)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return start.elapsed_time(end) / steps\n",
        "\n",
        "# Define shapes for benchmarking\n",
        "shapes = [\n",
        "    (4, 8, 64, 64),\n",
        "    (4, 16, 32, 32),\n",
        "    (8, 32, 64, 64),\n",
        "    (16, 64, 32, 32),\n",
        "]\n",
        "\n",
        "# Create random inputs\n",
        "images = [torch.randn(shape, device='cuda', dtype=torch.float32) for shape in shapes]\n",
        "gemmas = [torch.ones(shape[1], device='cuda', dtype=torch.float32) for shape in shapes]\n",
        "betas = [torch.zeros(shape[1], device='cuda', dtype=torch.float32) for shape in shapes]\n",
        "eps = 1e-5\n",
        "layers = [torch.nn.GroupNorm(4,shape[1],device='cuda',dtype=torch.float32) for shape in shapes]\n",
        "\n",
        "triton_time = [benchmark(groupnorm, image,4, gemma, beta, eps) for image, gemma, beta in zip(images, gemmas, betas)]\n",
        "\n",
        "layers = [torch.nn.BatchNorm2d(shape[1], device='cuda', dtype=torch.float32) for shape in shapes]\n",
        "\n",
        "for layer in layers:\n",
        "    layer.eval()\n",
        "\n",
        "torch_time = [benchmark(lambda x, g, b, layer=layer: layer(x), image, gemma, beta)\n",
        "              for layer, image, gemma, beta in zip(layers, images, gemmas, betas)]\n",
        "\n",
        "table = PrettyTable()\n",
        "table.field_names = [\"Shape\", \"Triton Time (ms)\", \"PyTorch Time (ms)\"]\n",
        "\n",
        "for shape, triton_t, torch_t in zip(shapes, triton_time, torch_time):\n",
        "    table.add_row([shape, f\"{triton_t:.4f}\", f\"{torch_t:.4f}\"])\n",
        "\n",
        "print(table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiUOvxbX0P8l",
        "outputId": "aad2d0ce-bbe5-45b2-ca1e-8f3b39851486"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+------------------+-------------------+\n",
            "|      Shape       | Triton Time (ms) | PyTorch Time (ms) |\n",
            "+------------------+------------------+-------------------+\n",
            "|  (4, 8, 64, 64)  |      0.0373      |       0.0529      |\n",
            "| (4, 16, 32, 32)  |      0.0528      |       0.0522      |\n",
            "| (8, 32, 64, 64)  |      0.0599      |       0.0559      |\n",
            "| (16, 64, 32, 32) |      0.0625      |       0.0520      |\n",
            "+------------------+------------------+-------------------+\n"
          ]
        }
      ]
    }
  ]
}