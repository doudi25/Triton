{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doudi25/Triton/blob/main/Layer_Norm_Forward.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMI35YAFEcgK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import triton\n",
        "import triton.language as tl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4guK5bAbYaP"
      },
      "outputs": [],
      "source": [
        "input = 10 * torch.rand((256,16),dtype=torch.float32,device='cuda')\n",
        "layer_norm = nn.LayerNorm(16,device='cuda')\n",
        "input_normalized = layer_norm(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynAg9GKoR89l"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVoRBjhyRB7c"
      },
      "outputs": [],
      "source": [
        "@triton.jit\n",
        "def forward_layer_norm(x_ptr,x_norm_ptr,gamma_ptr,beta_ptr,mean_ptr,var_ptr,eps,M:tl.constexpr,N:tl.constexpr,stride_xm,stride_xn,BLOCK_SIZE:tl.constexpr):\n",
        "  pid = tl.program_id(axis=0)\n",
        "  block_id = pid * BLOCK_SIZE\n",
        "  offs_row = block_id + tl.arange(0,BLOCK_SIZE)\n",
        "  offs_col = tl.arange(0,N)\n",
        "  mask = (offs_row[:,None] < M) & (offs_col[None,:] < N)\n",
        "  x_ptrs = x_ptr + offs_row[:,None] * stride_xm + offs_col[None,:] * stride_xn\n",
        "  gamma_ptrs = gamma_ptr + offs_col[None,:]\n",
        "  beta_ptrs = beta_ptr + offs_col[None,:]\n",
        "  x = tl.load(x_ptrs,mask=mask)\n",
        "  gamma = tl.load(gamma_ptrs)\n",
        "  beta = tl.load(beta_ptrs)\n",
        "  mean = tl.sum(x,axis=1,keep_dims=True) / N\n",
        "  mean = mean.to(tl.float32)\n",
        "  var = (tl.sum((x-mean)* (x-mean),axis=1,keep_dims=True) / N)\n",
        "  var = var.to(tl.float32)\n",
        "  x_norm = (x-mean) / (tl.sqrt(var + eps))\n",
        "  x_norm = x_norm * gamma + beta\n",
        "  tl.store(x_norm_ptr + offs_row[:,None] * stride_xm + offs_col[None,:] * stride_xn,x_norm,mask=mask)\n",
        "  tl.store(mean_ptr + offs_row,tl.reshape(mean,[BLOCK_SIZE]),mask=offs_row < M)\n",
        "  tl.store(var_ptr + offs_row,tl.reshape(var,[BLOCK_SIZE]),mask=offs_row < M) # Assuming rstd_ptr was intended, not std_ptr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MDtZMLxGCpF"
      },
      "outputs": [],
      "source": [
        "def _layer_norm_forward(x,gamma,beta,eps):\n",
        "  assert x.shape[1] == gamma.shape[0] , f'Incompatible shape'\n",
        "  assert x.shape[1] == beta.shape[0] , f'Incompatible shape'\n",
        "  assert x.is_contiguous(), f'x is not contiguous tensor'\n",
        "  rows , cols = x.shape\n",
        "  mean = torch.empty((rows,1),device=x.device,dtype=x.dtype)\n",
        "  var = torch.empty_like(mean,device=x.device,dtype=x.dtype)\n",
        "  x_norm = torch.empty_like(x,device=x.device,dtype=x.dtype)\n",
        "  grid = lambda meta : (triton.cdiv(rows,meta['BLOCK_SIZE']),)\n",
        "  # since the stride of vector with one dim is one we do not need to pass it , for mean , std , gamma , beta\n",
        "  forward_layer_norm[grid](x,\n",
        "                           x_norm,\n",
        "                           gamma,\n",
        "                           beta,\n",
        "                           mean,\n",
        "                           var,\n",
        "                           eps,\n",
        "                           rows,\n",
        "                           cols,\n",
        "                           x.stride(0),\n",
        "                           x.stride(1),\n",
        "                           BLOCK_SIZE=128,)\n",
        "  return x_norm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-FjM7V4xRNx"
      },
      "outputs": [],
      "source": [
        "input_norm  = _layer_norm_forward(input,layer_norm.weight,layer_norm.bias,layer_norm.eps)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.allclose(input_normalized,input_norm,1e-4))"
      ],
      "metadata": {
        "id": "KSX7fd267pW6",
        "outputId": "3bf31077-cf94-47c0-b502-fea6f0d0dbcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}