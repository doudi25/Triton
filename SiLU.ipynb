{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOv4K4COIASEZkA8LsHJAlX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doudi25/Triton/blob/main/SiLU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl"
      ],
      "metadata": {
        "id": "63FBVXhK8NE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _forward_kernel(x_ptr,y_ptr,stride_m,stride_n,m,n,BLOCK_SIZE_ROW:tl.constexpr,BLOCK_SIZE_COL:tl.constexpr,num_warps=64):\n",
        "  pid_m = tl.program_id(axis=0)\n",
        "  pid_n = tl.program_id(axis=1)\n",
        "  offs_m = pid_m * BLOCK_SIZE_ROW + tl.arange(0,BLOCK_SIZE_ROW)\n",
        "  offs_n = pid_n * BLOCK_SIZE_COL + tl.arange(0,BLOCK_SIZE_COL)\n",
        "  # apply mask\n",
        "  mask = (offs_m[:,None] < m ) & (offs_n[None,:] < n)\n",
        "  # assign correct accesing to x\n",
        "  x_ptrs = x_ptr + offs_m[:,None] * stride_m + offs_n[None,:] * stride_n\n",
        "  x = tl.load(x_ptrs,mask=mask)\n",
        "  # silu = x * sigmoid(x)\n",
        "  y = x * tl.sigmoid(x)\n",
        "  # assing out_ptrs\n",
        "  y_ptrs = y_ptr + offs_m[:,None] * stride_m + offs_n[None,:] * stride_n\n",
        "  # store result\n",
        "  tl.store(y_ptrs,y,mask=mask)"
      ],
      "metadata": {
        "id": "OIkowop08ODd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HozR728W5jeI"
      },
      "outputs": [],
      "source": [
        "def _forward_Silu(x:torch.tensor):\n",
        "  assert x.is_cuda and x.is_contiguous()\n",
        "  if x.ndim == 2:\n",
        "    m,n = x.shape\n",
        "  else:\n",
        "    x = x.view(-1,x.shape[-1])\n",
        "    m,n = x.shape\n",
        "  y = torch.empty_like(x,device=x.device,dtype=x.dtype)\n",
        "  BLOCK_SIZE_ROW = 32\n",
        "  BLOCK_SIZE_COL = 64\n",
        "  grid = (triton.cdiv(m,BLOCK_SIZE_ROW),triton.cdiv(n,BLOCK_SIZE_COL))\n",
        "  _forward_kernel[grid](x,y,x.stride(0),x.stride(1),m,n,BLOCK_SIZE_ROW,BLOCK_SIZE_COL)\n",
        "  return y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@triton.jit\n",
        "def _backward_kernel(x_ptr,dx_ptr,dout_ptr,stride_m,stride_n,\n",
        "                     m,n,BLOCK_SIZE_ROW:tl.constexpr,BLOCK_SIZE_COL:tl.constexpr,num_warps=64):\n",
        "  pid_m = tl.program_id(axis=0)\n",
        "  pid_n = tl.program_id(axis=1)\n",
        "  offs_m = pid_m * BLOCK_SIZE_ROW + tl.arange(0,BLOCK_SIZE_ROW)\n",
        "  offs_n = pid_n * BLOCK_SIZE_COL + tl.arange(0,BLOCK_SIZE_COL)\n",
        "  mask = (offs_m[:,None] < m ) & (offs_n[None,:] < n)\n",
        "  x_ptrs = x_ptr + offs_m[:,None] * stride_m + offs_n[None,:] * stride_n\n",
        "  x = tl.load(x_ptrs,mask=mask)\n",
        "  # assign dout ptrs\n",
        "  dout_ptrs = dout_ptr + offs_m[:,None] * stride_m + offs_n[None,:] * stride_\n",
        "  # load dout\n",
        "  dout = tl.load(dout_ptrs,mask=mask)\n",
        "  # dx = dout * ( sig(x) + dsigmoid(x) * x)\n",
        "  # dsimgoid(x) = sig(x) * ( 1 - sig(x))\n",
        "  sig = tl.sigmoid(x)\n",
        "  dx = dout * ( sig + (sig * ( 1 - sig )) * x)\n",
        "  # assign dx_ptrs\n",
        "  dx_ptrs = dx_ptr + offs_m[:,None] * stride_m + offs_n[None,:] * stride_n\n",
        "  # store gradient\n",
        "  tl.store(dx_ptrs,dx,mask=mask)\n",
        "\n"
      ],
      "metadata": {
        "id": "JD-JhoBH-7xG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _backward_Silu(x:torch.tensor,dout:torch.tensor):\n",
        "  assert x.is_cuda and dout.is_cuda\n",
        "  # make dout contiguous\n",
        "  dout = dout.contiguous()\n",
        "  assert x.is_contiguous() and dout.is_contiguous(),print(f'x is contiguous {x.is_contiguous()} , dout is contiguous {dout.is_contiguous}')\n",
        "  m,n = x.shape\n",
        "  dx = torch.empty_like(x,device=x.device,dtype=x.dtype)\n",
        "  BLOCK_SIZE_ROW = 32\n",
        "  BLOCK_SIZE_COL = 64\n",
        "  grid = (triton.cdiv(m,BLOCK_SIZE_ROW),triton.cdiv(n,BLOCK_SIZE_COL))\n",
        "  _backward_kernel[grid](x,dx,dout,x.stride(0),x.stride(1),m,n,BLOCK_SIZE_ROW,BLOCK_SIZE_COL)\n",
        "  return dx"
      ],
      "metadata": {
        "id": "uGpGQ5Yw9bgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Silu(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx,input):\n",
        "    ctx.save_for_backward(input)\n",
        "    out = _forward_Silu(input)\n",
        "    return out\n",
        "  @staticmethod\n",
        "  def backward(ctx,dout):\n",
        "    input = ctx.saved_tensors[0]\n",
        "    dx = _backward_Silu(input,dout)\n",
        "    return dx"
      ],
      "metadata": {
        "id": "CXb8BZHJ97Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_correctness():\n",
        "  input = torch.rand((1024,2048),device='cuda',requires_grad=True)\n",
        "  out_torch = torch.nn.functional.silu(input)\n",
        "  loss = out_torch.sum()\n",
        "  loss.backward()\n",
        "  # get the gradient of input using torch autograd enginee and remake it None for triton gradient\n",
        "  dinput_torch,input.grad = input.grad.clone(),None\n",
        "  # get output of triton kernel\n",
        "  out_triton = Silu.apply(input)\n",
        "  loss = out_triton.sum()\n",
        "  loss.backward()\n",
        "  # get the gradient of input using triton backward kernel\n",
        "  dinput_triton,input.grad = input.grad.clone(),None\n",
        "  # print result\n",
        "  return print(torch.allclose(dinput_triton,dinput_torch),\"The gradient of triton silu kernel is similar to pytorch autograd engine\")"
      ],
      "metadata": {
        "id": "AUVr4MpeDS2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "  test_correctness()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oE2X7FSDlf9",
        "outputId": "ae6acea7-86f2-4b34-b67a-e81e47857984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True The gradient of triton silu kernel is similar to pytorch autograd engine\n"
          ]
        }
      ]
    }
  ]
}